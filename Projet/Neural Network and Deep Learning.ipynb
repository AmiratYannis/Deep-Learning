{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rapport de Deep Learning</h1>\n",
    "<h2>Implementation d'un reseau de neurones pour classifier des chiffres en écriture manuscrite</h2>\n",
    "<h3> Yannis Amirat - Cyril Lagelée - ESIEE 2021 E4FI</h3>\n",
    "\n",
    "\n",
    "\n",
    "Dans ce projet, nous suivons les instructions d'un livre dématérialisé situé à l'adresse http://neuralnetworksanddeeplearning.com afin de comprendre comment est implémenté un reseau de neurones de Deep Learning afin de classifier des chiffres en écriture manuscrite. \n",
    "Nous allons transcrire notre progréssion au cours de ce projet dans ce jupyter notebook afin que le lecteur puisse reproduire notre parcours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au cours de ce livre, après une introduction à la structure des réseau de neurones et à une implémentation de descente de gradient, l'auteur nous redirige vers son git contenant les codes afin de suivre le déroulement de cette lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importation des codes fournies pour le projet</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import mnist_loader\n",
    "import network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy est une bibliothèque d'outils mathématique commune à Python. \n",
    "\n",
    "Random est un outil permet de générer des éléments pseudo-aléatoire. \n",
    "\n",
    "mnist_loader est un outil pour charger les données avec laquelle on va travailler provenant de Mixed National Institute of Standards and Technology. Ces données sont des scans de chiffres écrits à la main de 784 pixels.\n",
    "\n",
    "Network est une classe représentant une première forme d'implémentation d'un réseau de neurone. Dans cette classe est présent une implémentation d'un algorithme de descente de gradient stochastique. Dans cette implémentation, le poids et le biais sont initialisés aléatoirement pour démarrer l'algortithme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initiation au réseau de neurones</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous faire comprendre comment fonctionne cette implémentation de réseau de neurones, l'auteur nous fait construire un réseau de neurone en guise d'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 9026 / 10000\n",
      "Epoch 1 : 9267 / 10000\n",
      "Epoch 2 : 9295 / 10000\n",
      "Epoch 3 : 9327 / 10000\n",
      "Epoch 4 : 9369 / 10000\n",
      "Epoch 5 : 9399 / 10000\n",
      "Epoch 6 : 9392 / 10000\n",
      "Epoch 7 : 9388 / 10000\n",
      "Epoch 8 : 9409 / 10000\n",
      "Epoch 9 : 9402 / 10000\n",
      "Epoch 10 : 9430 / 10000\n",
      "Epoch 11 : 9434 / 10000\n",
      "Epoch 12 : 9421 / 10000\n",
      "Epoch 13 : 9437 / 10000\n",
      "Epoch 14 : 9448 / 10000\n",
      "Epoch 15 : 9443 / 10000\n",
      "Epoch 16 : 9483 / 10000\n",
      "Epoch 17 : 9462 / 10000\n",
      "Epoch 18 : 9461 / 10000\n",
      "Epoch 19 : 9501 / 10000\n",
      "Epoch 20 : 9489 / 10000\n",
      "Epoch 21 : 9480 / 10000\n",
      "Epoch 22 : 9464 / 10000\n",
      "Epoch 23 : 9469 / 10000\n",
      "Epoch 24 : 9467 / 10000\n",
      "Epoch 25 : 9465 / 10000\n",
      "Epoch 26 : 9485 / 10000\n",
      "Epoch 27 : 9490 / 10000\n",
      "Epoch 28 : 9470 / 10000\n",
      "Epoch 29 : 9483 / 10000\n"
     ]
    }
   ],
   "source": [
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "hidden_layer1=30 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=30 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=3.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le livre, l'auteur obtient un pourcentage de 95,42% à son meilleur epoch qu'il trouve prometteur. De notre coté, nous obtenons un résultat légérement différent en raison des biais et des poids aléatoire à l'initialisation. \n",
    "Ce qui nous donne 94,94% à l'epoch 22 ce qui représente en effet un bon résultat de reconnaissance de chiffres sur 10000 chiffres.\n",
    "Suite à celà, l'auteur nous propose de tester avec 100 neurones sur la couche cachée et différents learning rate afin d'en observer les résultats. \n",
    "\n",
    "Nous commençons avec un learning rate de 0.001 que l'on va faire progressivement augmenter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 1035 / 10000\n",
      "Epoch 1 : 1039 / 10000\n",
      "Epoch 2 : 1165 / 10000\n",
      "Epoch 3 : 1482 / 10000\n",
      "Epoch 4 : 1626 / 10000\n",
      "Epoch 5 : 1716 / 10000\n",
      "Epoch 6 : 1808 / 10000\n",
      "Epoch 7 : 1855 / 10000\n",
      "Epoch 8 : 1895 / 10000\n",
      "Epoch 9 : 1920 / 10000\n",
      "Epoch 10 : 1954 / 10000\n",
      "Epoch 11 : 1997 / 10000\n",
      "Epoch 12 : 2028 / 10000\n",
      "Epoch 13 : 2056 / 10000\n",
      "Epoch 14 : 2082 / 10000\n",
      "Epoch 15 : 2111 / 10000\n",
      "Epoch 16 : 2140 / 10000\n",
      "Epoch 17 : 2162 / 10000\n",
      "Epoch 18 : 2192 / 10000\n",
      "Epoch 19 : 2231 / 10000\n",
      "Epoch 20 : 2261 / 10000\n",
      "Epoch 21 : 2276 / 10000\n",
      "Epoch 22 : 2305 / 10000\n",
      "Epoch 23 : 2327 / 10000\n",
      "Epoch 24 : 2346 / 10000\n",
      "Epoch 25 : 2366 / 10000\n",
      "Epoch 26 : 2392 / 10000\n",
      "Epoch 27 : 2417 / 10000\n",
      "Epoch 28 : 2430 / 10000\n",
      "Epoch 29 : 2450 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.001 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme l'auteur l'a suggéré des paramètres incorrectement défini peut causer une lenteur de l'execution mais aussi de très mauvais résultat. \n",
    "Malgré l'ajout de 70 neurones à notre couche caché, le meilleur résultat qu'on a obtenu est de l'ordre de 22,62% avec un learning rate de 0.001. \n",
    "Ce dernier suggère que notre learning rate est en effet la source de nos faibles résultats et suggère d'effectuer plusieurs executions en augmentant le learning rate et d'en observer les résultats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 1209 / 10000\n",
      "Epoch 1 : 1340 / 10000\n",
      "Epoch 2 : 1446 / 10000\n",
      "Epoch 3 : 1503 / 10000\n",
      "Epoch 4 : 1561 / 10000\n",
      "Epoch 5 : 1597 / 10000\n",
      "Epoch 6 : 1651 / 10000\n",
      "Epoch 7 : 1691 / 10000\n",
      "Epoch 8 : 1757 / 10000\n",
      "Epoch 9 : 1809 / 10000\n",
      "Epoch 10 : 1863 / 10000\n",
      "Epoch 11 : 1951 / 10000\n",
      "Epoch 12 : 2017 / 10000\n",
      "Epoch 13 : 2077 / 10000\n",
      "Epoch 14 : 2125 / 10000\n",
      "Epoch 15 : 2174 / 10000\n",
      "Epoch 16 : 2239 / 10000\n",
      "Epoch 17 : 2271 / 10000\n",
      "Epoch 18 : 2322 / 10000\n",
      "Epoch 19 : 2370 / 10000\n",
      "Epoch 20 : 2422 / 10000\n",
      "Epoch 21 : 2486 / 10000\n",
      "Epoch 22 : 2537 / 10000\n",
      "Epoch 23 : 2588 / 10000\n",
      "Epoch 24 : 2658 / 10000\n",
      "Epoch 25 : 2726 / 10000\n",
      "Epoch 26 : 2790 / 10000\n",
      "Epoch 27 : 2860 / 10000\n",
      "Epoch 28 : 2912 / 10000\n",
      "Epoch 29 : 2960 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.01 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.001 à 0.01. Nous pouvons observer une progréssion significatif de notre pourcentage d'environ 24% qui est dorénavant de 46.75%. Nous pouvons supposer qu'une augmentation du learning rate augmentera ce pourcentage de nouveau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 3962 / 10000\n",
      "Epoch 1 : 5337 / 10000\n",
      "Epoch 2 : 6172 / 10000\n",
      "Epoch 3 : 6727 / 10000\n",
      "Epoch 4 : 6984 / 10000\n",
      "Epoch 5 : 7139 / 10000\n",
      "Epoch 6 : 7247 / 10000\n",
      "Epoch 7 : 7378 / 10000\n",
      "Epoch 8 : 7524 / 10000\n",
      "Epoch 9 : 7790 / 10000\n",
      "Epoch 10 : 7943 / 10000\n",
      "Epoch 11 : 8004 / 10000\n",
      "Epoch 12 : 8055 / 10000\n",
      "Epoch 13 : 8094 / 10000\n",
      "Epoch 14 : 8125 / 10000\n",
      "Epoch 15 : 8148 / 10000\n",
      "Epoch 16 : 8169 / 10000\n",
      "Epoch 17 : 8183 / 10000\n",
      "Epoch 18 : 8200 / 10000\n",
      "Epoch 19 : 8225 / 10000\n",
      "Epoch 20 : 8239 / 10000\n",
      "Epoch 21 : 8246 / 10000\n",
      "Epoch 22 : 8258 / 10000\n",
      "Epoch 23 : 8273 / 10000\n",
      "Epoch 24 : 8279 / 10000\n",
      "Epoch 25 : 8295 / 10000\n",
      "Epoch 26 : 8299 / 10000\n",
      "Epoch 27 : 8316 / 10000\n",
      "Epoch 28 : 8330 / 10000\n",
      "Epoch 29 : 8338 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.1 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.01 à 0.1. Nous pouvons observer encore une progréssion significatif de notre pourcentage d'environ 28% qui est dorénavant de 74.96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 6085 / 10000\n",
      "Epoch 1 : 6447 / 10000\n",
      "Epoch 2 : 6558 / 10000\n",
      "Epoch 3 : 6650 / 10000\n",
      "Epoch 4 : 8384 / 10000\n",
      "Epoch 5 : 8474 / 10000\n",
      "Epoch 6 : 9351 / 10000\n",
      "Epoch 7 : 9415 / 10000\n",
      "Epoch 8 : 9470 / 10000\n",
      "Epoch 9 : 9481 / 10000\n",
      "Epoch 10 : 9514 / 10000\n",
      "Epoch 11 : 9516 / 10000\n",
      "Epoch 12 : 9525 / 10000\n",
      "Epoch 13 : 9537 / 10000\n",
      "Epoch 14 : 9537 / 10000\n",
      "Epoch 15 : 9555 / 10000\n",
      "Epoch 16 : 9553 / 10000\n",
      "Epoch 17 : 9554 / 10000\n",
      "Epoch 18 : 9569 / 10000\n",
      "Epoch 19 : 9563 / 10000\n",
      "Epoch 20 : 9558 / 10000\n",
      "Epoch 21 : 9570 / 10000\n",
      "Epoch 22 : 9574 / 10000\n",
      "Epoch 23 : 9585 / 10000\n",
      "Epoch 24 : 9582 / 10000\n",
      "Epoch 25 : 9583 / 10000\n",
      "Epoch 26 : 9580 / 10000\n",
      "Epoch 27 : 9591 / 10000\n",
      "Epoch 28 : 9585 / 10000\n",
      "Epoch 29 : 9588 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=1.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.1 à 1.0 Nous pouvons observer encore une progréssion significatif de notre pourcentage d'environ 11% qui est dorénavant de 86.74%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 7129 / 10000\n",
      "Epoch 1 : 8143 / 10000\n",
      "Epoch 2 : 9370 / 10000\n",
      "Epoch 3 : 9428 / 10000\n",
      "Epoch 4 : 9450 / 10000\n",
      "Epoch 5 : 9467 / 10000\n",
      "Epoch 6 : 9513 / 10000\n",
      "Epoch 7 : 9521 / 10000\n",
      "Epoch 8 : 9557 / 10000\n",
      "Epoch 9 : 9567 / 10000\n",
      "Epoch 10 : 9560 / 10000\n",
      "Epoch 11 : 9571 / 10000\n",
      "Epoch 12 : 9583 / 10000\n",
      "Epoch 13 : 9587 / 10000\n",
      "Epoch 14 : 9582 / 10000\n",
      "Epoch 15 : 9589 / 10000\n",
      "Epoch 16 : 9600 / 10000\n",
      "Epoch 17 : 9609 / 10000\n",
      "Epoch 18 : 9596 / 10000\n",
      "Epoch 19 : 9596 / 10000\n",
      "Epoch 20 : 9610 / 10000\n",
      "Epoch 21 : 9622 / 10000\n",
      "Epoch 22 : 9608 / 10000\n",
      "Epoch 23 : 9617 / 10000\n",
      "Epoch 24 : 9607 / 10000\n",
      "Epoch 25 : 9607 / 10000\n",
      "Epoch 26 : 9620 / 10000\n",
      "Epoch 27 : 9613 / 10000\n",
      "Epoch 28 : 9618 / 10000\n",
      "Epoch 29 : 9609 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=2.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 1.0 à 2.0 Nous pouvons observer encore une progréssion significatif de notre pourcentage d'environ 9% qui est dorénavant de 95.97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 7521 / 10000\n",
      "Epoch 1 : 7628 / 10000\n",
      "Epoch 2 : 7654 / 10000\n",
      "Epoch 3 : 7718 / 10000\n",
      "Epoch 4 : 8502 / 10000\n",
      "Epoch 5 : 8563 / 10000\n",
      "Epoch 6 : 8597 / 10000\n",
      "Epoch 7 : 8628 / 10000\n",
      "Epoch 8 : 8635 / 10000\n",
      "Epoch 9 : 8648 / 10000\n",
      "Epoch 10 : 8647 / 10000\n",
      "Epoch 11 : 8698 / 10000\n",
      "Epoch 12 : 8985 / 10000\n",
      "Epoch 13 : 9574 / 10000\n",
      "Epoch 14 : 9568 / 10000\n",
      "Epoch 15 : 9576 / 10000\n",
      "Epoch 16 : 9569 / 10000\n",
      "Epoch 17 : 9552 / 10000\n",
      "Epoch 18 : 9558 / 10000\n",
      "Epoch 19 : 9592 / 10000\n",
      "Epoch 20 : 9597 / 10000\n",
      "Epoch 21 : 9596 / 10000\n",
      "Epoch 22 : 9606 / 10000\n",
      "Epoch 23 : 9605 / 10000\n",
      "Epoch 24 : 9574 / 10000\n",
      "Epoch 25 : 9603 / 10000\n",
      "Epoch 26 : 9614 / 10000\n",
      "Epoch 27 : 9607 / 10000\n",
      "Epoch 28 : 9619 / 10000\n",
      "Epoch 29 : 9615 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=3.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 2.0 à 3.0. Nous pouvons observer encore une régréssion de notre pourcentage d'environ 11% qui est dorénavant de 84.64%. \n",
    "\n",
    "L'hypothèse que l'on peu émettre sur ces resultats est que les variations de learning rate, de poids aléatoire et de biais aléatoire peuvent avoir un impact significatif sur les résultats comme l'auteur souhaite nous le démontrer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Exercice\n",
    "\n",
    "Essayer de crée un reseau avec deux layer , un input et un output sans reseau cacher avec 784 et 10 neurones,chacun. Entrainner le reseau avec la descente de gradiant stocastique. Quelle présision de classification pouvez-vous obtenir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 6560 / 10000\n",
      "Epoch 1 : 7314 / 10000\n",
      "Epoch 2 : 7418 / 10000\n",
      "Epoch 3 : 7431 / 10000\n",
      "Epoch 4 : 7427 / 10000\n",
      "Epoch 5 : 7460 / 10000\n",
      "Epoch 6 : 7494 / 10000\n",
      "Epoch 7 : 7517 / 10000\n",
      "Epoch 8 : 7501 / 10000\n",
      "Epoch 9 : 7498 / 10000\n",
      "Epoch 10 : 7492 / 10000\n",
      "Epoch 11 : 7502 / 10000\n",
      "Epoch 12 : 7518 / 10000\n",
      "Epoch 13 : 7506 / 10000\n",
      "Epoch 14 : 7522 / 10000\n",
      "Epoch 15 : 7519 / 10000\n",
      "Epoch 16 : 7520 / 10000\n",
      "Epoch 17 : 7565 / 10000\n",
      "Epoch 18 : 8378 / 10000\n",
      "Epoch 19 : 8393 / 10000\n",
      "Epoch 20 : 8372 / 10000\n",
      "Epoch 21 : 8375 / 10000\n",
      "Epoch 22 : 8366 / 10000\n",
      "Epoch 23 : 8381 / 10000\n",
      "Epoch 24 : 8396 / 10000\n",
      "Epoch 25 : 8387 / 10000\n",
      "Epoch 26 : 8390 / 10000\n",
      "Epoch 27 : 8377 / 10000\n",
      "Epoch 28 : 8423 / 10000\n",
      "Epoch 29 : 8405 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=2.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un resultat de 83,26%. Ce qui nous semble étrange au vue de la structure de notre reseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vers des methodes moins naif</h1>\n",
    "\n",
    "Dans la suite du livre, l'auteur nous présente l'implementation de mnist_loader qui en plus de chargé données, ce programme structure les données afin d'etre correctement interpréter par le reseau de neurones.\n",
    "\n",
    "En suite ce derniers, nous rappele que notre methodes est naif et qu'elle manque de methodologie pour comparer nos résultats. En effet nous pouvons comparer avec notre propre reseau mais l'idéal serait de pouvoir comparer avec d'autres methodes non basé sur les reseaux de neurones. Pour cela, il suggère d'utiliser une implémentation d'un des algorithmes les plus connue avec ses paramètres par défauts : support vector machine ou SVM afin de comparer.\n",
    "Ce dernier devrait avoir un resultat de environ 94.35% par defaut et environ 98.5% lorsque qu'il est bien paramètrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classifier using an SVM.\n",
      "9785 of 10000 values correct.\n"
     ]
    }
   ],
   "source": [
    "mnist_svm.svm_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme annoncer par l'auteur, svm un très bon résultat de 97,85%. Cela semble à première vue compliqué à rivaliser mais un reseau de neurones de 2013 fût capable de battre avec 99.79%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep learning</h2>\n",
    "    \n",
    "Comme le signal l'auteur, le fait que l'on ne connaisse pas les poids et les biais donner a nos algorithmes crée un  phénomène souvent repprocher au methode de machine learning l'aspect de boite noire et prend des données et qui en renvoie d'autre, diminuant ainsi notre compréhention que l'on peu en tirer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous permettre de visualiser le concept de \"reseau de neurones profond\", il nous présente l'idée de comment ont pourrait chercher a resoudre la question \"l'image contient t'elle un visage?\" par un reseau de neurone. \n",
    "Ainsi le problème pourrait etre decouper en de nombreuses sous-questions tel que \"un nez est t'il présent?\" et donc pour chacune de ses sous-question un sous-reseau pour y repondre au sein de grand reseau composer de nombreuse couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de poursuivre l'auteur a souhaiter permettre aux lecteurs la possiblilité de d'ouvrir cette \"boite noire\" de d'aider et d'accompagner le lecteur dans la compréhention de l'algorime de rétropropagation un des pilliers permettrant avoir des reseaux plus rapides, plus large et donc plus puissant. \n",
    "Cependant nous l'allons pas tenter ce chapitre qui bien que interressant, il est beaucoup orienter \"lourd\" mathématiquement pour etre expliquer aussi aisaiment que l'auteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importation de Network2, une amelioration de la précédente classe</h2>\n",
    "\n",
    "Avec cette nouvelle classe l'auteur apporte des modifications a la classe network précédente pour integrer des donnée associer a la rétropropagation avec une classe associer a la fonction de Cout et une modification de comment sont initialiser les poids ainsi que les biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network2 as n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous présenter sa nouvelle classe l'auteur nous propose d'experimenter une situation où ont souhaite a resoudre un nouveau problème depuit le debut sans qu'on est les pistes initial sur la structure de notre reseau et de ses paramètres. Pour cela, il propose a commencer simple avec un reseau sans couche cacher et une petite quantité de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: inf\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: inf\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 1 training complete\n",
      "Cost on training data: inf\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: inf\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 2 training complete\n",
      "Cost on training data: inf\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: inf\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 3 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 4 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 5 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 6 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 7 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 8 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 9 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 10 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 11 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 12 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 13 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 14 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 15 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 16 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 17 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 18 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 19 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 20 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 21 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 22 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 23 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 24 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 25 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 26 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 27 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 28 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n",
      "Epoch 29 training complete\n",
      "Cost on training data: nan\n",
      "Accuracy on training data: 97 / 1000\n",
      "Cost on evaluation data: nan\n",
      "Accuracy on evaluation data: 10 / 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "#hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=30 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=10.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "lmbda2 = 1000.0\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "test_data = list(test_data)\n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net2 = n2.Network([input_layer, output_layer])\n",
    "\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net2.SGD(training_data[:1000], epoch, size_batch, learning_rate, lmbda = lmbda2,evaluation_data=validation_data[:100],monitor_evaluation_accuracy=True,monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
