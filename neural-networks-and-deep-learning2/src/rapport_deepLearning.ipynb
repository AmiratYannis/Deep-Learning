{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rapport de Deep Learning</h1>\n",
    "<h2>Implementation d'un reseau de neurones pour classifier des chiffres en écriture manuscrite</h2>\n",
    "<h3> Yannis Amirat - Cyril Lagelée - ESIEE 2021 E4FI</h3>\n",
    "\n",
    "\n",
    "\n",
    "Dans ce projet, nous suivons les instructions d'un livre dématérialisé situé à l'adresse http://neuralnetworksanddeeplearning.com afin de comprendre comment est implémenté un reseau de neurones de Deep Learning afin de classifier des chiffres en écriture manuscrite. \n",
    "Nous allons transcrire notre progréssion au cours de ce projet dans ce jupyter notebook afin que le lecteur puisse reproduire notre parcours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au cours de ce livre, après une introduction à la structure des réseau de neurones et à une implémentation de descente de gradient, l'auteur nous redirige vers son git contenant les codes afin de suivre le déroulement de cette lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importation des codes fournies pour le projet</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import mnist_loader\n",
    "import network\n",
    "import mnist_svm\n",
    "import network2 as n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy est une bibliothèque d'outils mathématique commune à Python. \n",
    "\n",
    "Random est un outil permet de générer des éléments pseudo-aléatoire. \n",
    "\n",
    "mnist_loader est un outil pour charger les données avec laquelle on va travailler provenant de Mixed National Institute of Standards and Technology. Ces données sont des scans de chiffres écrits à la main de 784 pixels.\n",
    "\n",
    "Network est une classe représentant une première forme d'implémentation d'un réseau de neurone. Dans cette classe est présent une implémentation d'un algorithme de descente de gradient stochastique. Dans cette implémentation, le poids et le biais sont initialisés aléatoirement pour démarrer l'algortithme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initiation au réseau de neurones</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous faire comprendre comment fonctionne cette implémentation de réseau de neurones, l'auteur nous fait construire un réseau de neurone en guise d'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9080 / 10000\n",
      "Epoch 1: 9192 / 10000\n",
      "Epoch 2: 9334 / 10000\n",
      "Epoch 3: 9368 / 10000\n",
      "Epoch 4: 9393 / 10000\n",
      "Epoch 5: 9402 / 10000\n",
      "Epoch 6: 9426 / 10000\n",
      "Epoch 7: 9467 / 10000\n",
      "Epoch 8: 9451 / 10000\n",
      "Epoch 9: 9434 / 10000\n",
      "Epoch 10: 9479 / 10000\n",
      "Epoch 11: 9473 / 10000\n",
      "Epoch 12: 9415 / 10000\n",
      "Epoch 13: 9460 / 10000\n",
      "Epoch 14: 9468 / 10000\n",
      "Epoch 15: 9480 / 10000\n",
      "Epoch 16: 9434 / 10000\n",
      "Epoch 17: 9486 / 10000\n",
      "Epoch 18: 9478 / 10000\n",
      "Epoch 19: 9482 / 10000\n",
      "Epoch 20: 9499 / 10000\n",
      "Epoch 21: 9505 / 10000\n",
      "Epoch 22: 9479 / 10000\n",
      "Epoch 23: 9461 / 10000\n",
      "Epoch 24: 9511 / 10000\n",
      "Epoch 25: 9416 / 10000\n",
      "Epoch 26: 9502 / 10000\n",
      "Epoch 27: 9501 / 10000\n",
      "Epoch 28: 9490 / 10000\n",
      "Epoch 29: 9503 / 10000\n"
     ]
    }
   ],
   "source": [
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "hidden_layer1=30 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=30 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=3.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le livre, l'auteur obtient un pourcentage de 95,42% à son meilleur epoch qu'il trouve prometteur. De notre coté, nous obtenons un résultat légérement différent en raison des biais et des poids aléatoire à l'initialisation. \n",
    "Ce qui nous donne 94,11% à l'epoch 24 ce qui représente en effet un bon résultat de reconnaissance de chiffres sur 10000 chiffres.\n",
    "Suite à celà, l'auteur nous propose de tester avec 100 neurones sur la couche cachée et différents learning rate afin d'en observer les résultats. \n",
    "\n",
    "Nous commençons avec un learning rate de 0.001 que l'on va faire progressivement augmenter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1211 / 10000\n",
      "Epoch 1: 1189 / 10000\n",
      "Epoch 2: 1201 / 10000\n",
      "Epoch 3: 1220 / 10000\n",
      "Epoch 4: 1253 / 10000\n",
      "Epoch 5: 1288 / 10000\n",
      "Epoch 6: 1322 / 10000\n",
      "Epoch 7: 1375 / 10000\n",
      "Epoch 8: 1415 / 10000\n",
      "Epoch 9: 1450 / 10000\n",
      "Epoch 10: 1483 / 10000\n",
      "Epoch 11: 1509 / 10000\n",
      "Epoch 12: 1520 / 10000\n",
      "Epoch 13: 1552 / 10000\n",
      "Epoch 14: 1583 / 10000\n",
      "Epoch 15: 1599 / 10000\n",
      "Epoch 16: 1622 / 10000\n",
      "Epoch 17: 1644 / 10000\n",
      "Epoch 18: 1668 / 10000\n",
      "Epoch 19: 1694 / 10000\n",
      "Epoch 20: 1737 / 10000\n",
      "Epoch 21: 1767 / 10000\n",
      "Epoch 22: 1798 / 10000\n",
      "Epoch 23: 1823 / 10000\n",
      "Epoch 24: 1870 / 10000\n",
      "Epoch 25: 1898 / 10000\n",
      "Epoch 26: 1939 / 10000\n",
      "Epoch 27: 1974 / 10000\n",
      "Epoch 28: 2016 / 10000\n",
      "Epoch 29: 2062 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.001 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme l'auteur l'a suggéré des paramètres incorrectement défini peut causer une lenteur de l'execution mais aussi de très mauvais résultat. \n",
    "Malgré l'ajout de 70 neurones à notre couche caché, le meilleur résultat qu'on a obtenu est de l'ordre de 20,62% avec un learning rate de 0.001. \n",
    "Ce dernier suggère que notre learning rate est en effet la source de nos faibles résultats et suggère d'effectuer plusieurs executions en augmentant le learning rate et d'en observer les résultats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1313 / 10000\n",
      "Epoch 1: 1518 / 10000\n",
      "Epoch 2: 1668 / 10000\n",
      "Epoch 3: 1773 / 10000\n",
      "Epoch 4: 1869 / 10000\n",
      "Epoch 5: 1929 / 10000\n",
      "Epoch 6: 1976 / 10000\n",
      "Epoch 7: 2012 / 10000\n",
      "Epoch 8: 2056 / 10000\n",
      "Epoch 9: 2086 / 10000\n",
      "Epoch 10: 2117 / 10000\n",
      "Epoch 11: 2160 / 10000\n",
      "Epoch 12: 2202 / 10000\n",
      "Epoch 13: 2248 / 10000\n",
      "Epoch 14: 2316 / 10000\n",
      "Epoch 15: 2438 / 10000\n",
      "Epoch 16: 2745 / 10000\n",
      "Epoch 17: 2945 / 10000\n",
      "Epoch 18: 3022 / 10000\n",
      "Epoch 19: 3086 / 10000\n",
      "Epoch 20: 3148 / 10000\n",
      "Epoch 21: 3199 / 10000\n",
      "Epoch 22: 3252 / 10000\n",
      "Epoch 23: 3311 / 10000\n",
      "Epoch 24: 3361 / 10000\n",
      "Epoch 25: 3427 / 10000\n",
      "Epoch 26: 3483 / 10000\n",
      "Epoch 27: 3553 / 10000\n",
      "Epoch 28: 3624 / 10000\n",
      "Epoch 29: 3693 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.01 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.001 à 0.01. Nous pouvons observer une progréssion de notre pourcentage d'environ 16.31% qui est dorénavant de 36,93%. Nous pouvons supposer qu'une augmentation du learning rate augmentera ce pourcentage de nouveau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 4528 / 10000\n",
      "Epoch 1: 6135 / 10000\n",
      "Epoch 2: 6960 / 10000\n",
      "Epoch 3: 7203 / 10000\n",
      "Epoch 4: 7310 / 10000\n",
      "Epoch 5: 7395 / 10000\n",
      "Epoch 6: 7426 / 10000\n",
      "Epoch 7: 7451 / 10000\n",
      "Epoch 8: 7489 / 10000\n",
      "Epoch 9: 7503 / 10000\n",
      "Epoch 10: 7523 / 10000\n",
      "Epoch 11: 7544 / 10000\n",
      "Epoch 12: 7551 / 10000\n",
      "Epoch 13: 7567 / 10000\n",
      "Epoch 14: 7572 / 10000\n",
      "Epoch 15: 7601 / 10000\n",
      "Epoch 16: 7605 / 10000\n",
      "Epoch 17: 7614 / 10000\n",
      "Epoch 18: 7623 / 10000\n",
      "Epoch 19: 7630 / 10000\n",
      "Epoch 20: 7640 / 10000\n",
      "Epoch 21: 7647 / 10000\n",
      "Epoch 22: 7656 / 10000\n",
      "Epoch 23: 7663 / 10000\n",
      "Epoch 24: 7662 / 10000\n",
      "Epoch 25: 7670 / 10000\n",
      "Epoch 26: 7675 / 10000\n",
      "Epoch 27: 7679 / 10000\n",
      "Epoch 28: 7684 / 10000\n",
      "Epoch 29: 7694 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.1 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.01 à 0.1. Nous pouvons observer encore une progréssion significatif de notre pourcentage d'environ 40.01% qui est dorénavant de 76.94%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7688 / 10000\n",
      "Epoch 1: 8248 / 10000\n",
      "Epoch 2: 8376 / 10000\n",
      "Epoch 3: 9233 / 10000\n",
      "Epoch 4: 9336 / 10000\n",
      "Epoch 5: 9390 / 10000\n",
      "Epoch 6: 9403 / 10000\n",
      "Epoch 7: 9435 / 10000\n",
      "Epoch 8: 9440 / 10000\n",
      "Epoch 9: 9452 / 10000\n",
      "Epoch 10: 9477 / 10000\n",
      "Epoch 11: 9494 / 10000\n",
      "Epoch 12: 9512 / 10000\n",
      "Epoch 13: 9526 / 10000\n",
      "Epoch 14: 9516 / 10000\n",
      "Epoch 15: 9528 / 10000\n",
      "Epoch 16: 9521 / 10000\n",
      "Epoch 17: 9539 / 10000\n",
      "Epoch 18: 9542 / 10000\n",
      "Epoch 19: 9563 / 10000\n",
      "Epoch 20: 9562 / 10000\n",
      "Epoch 21: 9554 / 10000\n",
      "Epoch 22: 9561 / 10000\n",
      "Epoch 23: 9559 / 10000\n",
      "Epoch 24: 9575 / 10000\n",
      "Epoch 25: 9567 / 10000\n",
      "Epoch 26: 9570 / 10000\n",
      "Epoch 27: 9562 / 10000\n",
      "Epoch 28: 9566 / 10000\n",
      "Epoch 29: 9566 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=1.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.1 à 1.0 Nous pouvons observer encore une progréssion de notre pourcentage d'environ 18.81% qui est dorénavant de 95.75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6481 / 10000\n",
      "Epoch 1: 7499 / 10000\n",
      "Epoch 2: 7584 / 10000\n",
      "Epoch 3: 7632 / 10000\n",
      "Epoch 4: 7726 / 10000\n",
      "Epoch 5: 8508 / 10000\n",
      "Epoch 6: 8560 / 10000\n",
      "Epoch 7: 8585 / 10000\n",
      "Epoch 8: 8600 / 10000\n",
      "Epoch 9: 8615 / 10000\n",
      "Epoch 10: 8634 / 10000\n",
      "Epoch 11: 8634 / 10000\n",
      "Epoch 12: 8649 / 10000\n",
      "Epoch 13: 8663 / 10000\n",
      "Epoch 14: 8671 / 10000\n",
      "Epoch 15: 8662 / 10000\n",
      "Epoch 16: 8677 / 10000\n",
      "Epoch 17: 8715 / 10000\n",
      "Epoch 18: 9580 / 10000\n",
      "Epoch 19: 9586 / 10000\n",
      "Epoch 20: 9592 / 10000\n",
      "Epoch 21: 9584 / 10000\n",
      "Epoch 22: 9581 / 10000\n",
      "Epoch 23: 9596 / 10000\n",
      "Epoch 24: 9593 / 10000\n",
      "Epoch 25: 9606 / 10000\n",
      "Epoch 26: 9598 / 10000\n",
      "Epoch 27: 9609 / 10000\n",
      "Epoch 28: 9614 / 10000\n",
      "Epoch 29: 9603 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=2.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 1.0 à 2.0 Nous pouvons observer encore une progréssion légère de notre pourcentage de moins de 1% qui est dorénavant de 96.14%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 5575 / 10000\n",
      "Epoch 1: 6628 / 10000\n",
      "Epoch 2: 6704 / 10000\n",
      "Epoch 3: 6773 / 10000\n",
      "Epoch 4: 7508 / 10000\n",
      "Epoch 5: 7617 / 10000\n",
      "Epoch 6: 7691 / 10000\n",
      "Epoch 7: 7716 / 10000\n",
      "Epoch 8: 7720 / 10000\n",
      "Epoch 9: 7780 / 10000\n",
      "Epoch 10: 7779 / 10000\n",
      "Epoch 11: 7743 / 10000\n",
      "Epoch 12: 7840 / 10000\n",
      "Epoch 13: 7943 / 10000\n",
      "Epoch 14: 7855 / 10000\n",
      "Epoch 15: 7887 / 10000\n",
      "Epoch 16: 8131 / 10000\n",
      "Epoch 17: 8676 / 10000\n",
      "Epoch 18: 8679 / 10000\n",
      "Epoch 19: 8680 / 10000\n",
      "Epoch 20: 8685 / 10000\n",
      "Epoch 21: 8685 / 10000\n",
      "Epoch 22: 8687 / 10000\n",
      "Epoch 23: 8683 / 10000\n",
      "Epoch 24: 8686 / 10000\n",
      "Epoch 25: 8682 / 10000\n",
      "Epoch 26: 8697 / 10000\n",
      "Epoch 27: 8689 / 10000\n",
      "Epoch 28: 8693 / 10000\n",
      "Epoch 29: 8705 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=3.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 2.0 à 3.0. Nous pouvons observer encore une régréssion de notre pourcentage d'environ 9% qui est dorénavant de 87.05%. \n",
    "\n",
    "L'hypothèse que l'on peu émettre sur ces resultats est que les variations de learning rate, de poids aléatoire et de biais aléatoire peuvent avoir un impact significatif sur les résultats comme l'auteur souhaite nous le démontrer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Exercice\n",
    "\n",
    "Essayer de crée un reseau avec deux layer , un input et un output sans reseau cacher avec 784 et 10 neurones,chacun. Entrainner le reseau avec la descente de gradiant stocastique. Quelle présision de classification pouvez-vous obtenir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6070 / 10000\n",
      "Epoch 1: 6263 / 10000\n",
      "Epoch 2: 6276 / 10000\n",
      "Epoch 3: 6309 / 10000\n",
      "Epoch 4: 6308 / 10000\n",
      "Epoch 5: 6326 / 10000\n",
      "Epoch 6: 6325 / 10000\n",
      "Epoch 7: 6315 / 10000\n",
      "Epoch 8: 6341 / 10000\n",
      "Epoch 9: 6332 / 10000\n",
      "Epoch 10: 6349 / 10000\n",
      "Epoch 11: 6361 / 10000\n",
      "Epoch 12: 6341 / 10000\n",
      "Epoch 13: 6386 / 10000\n",
      "Epoch 14: 7257 / 10000\n",
      "Epoch 15: 7295 / 10000\n",
      "Epoch 16: 8375 / 10000\n",
      "Epoch 17: 8401 / 10000\n",
      "Epoch 18: 8397 / 10000\n",
      "Epoch 19: 8387 / 10000\n",
      "Epoch 20: 8385 / 10000\n",
      "Epoch 21: 8373 / 10000\n",
      "Epoch 22: 8390 / 10000\n",
      "Epoch 23: 8386 / 10000\n",
      "Epoch 24: 8412 / 10000\n",
      "Epoch 25: 8411 / 10000\n",
      "Epoch 26: 8403 / 10000\n",
      "Epoch 27: 8425 / 10000\n",
      "Epoch 28: 8400 / 10000\n",
      "Epoch 29: 9017 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=2.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un resultat de 90.17% ce qui est un score plutôt bon pour un reseau composer de deux couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vers des methodes moins naif</h1>\n",
    "\n",
    "Dans la suite du livre, l'auteur nous présente l'implementation de mnist_loader qui en plus de chargé données, ce programme structure les données afin d'etre correctement interpréter par le reseau de neurones.\n",
    "\n",
    "En suite ce derniers, nous rappele que notre methodes est naif et qu'elle manque de methodologie pour comparer nos résultats. En effet nous pouvons comparer avec notre propre reseau mais l'idéal serait de pouvoir comparer avec d'autres methodes non basé sur les reseaux de neurones. Pour cela, il suggère d'utiliser une implémentation d'un des algorithmes les plus connue avec ses paramètres par défauts : support vector machine ou SVM afin de comparer.\n",
    "Ce dernier devrait avoir un resultat de environ 94.35% par defaut et environ 98.5% lorsque qu'il est bien paramètrer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda2\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classifier using an SVM.\n",
      "9435 of 10000 values correct.\n"
     ]
    }
   ],
   "source": [
    "mnist_svm.svm_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme annoncer par l'auteur, svm un très bon résultat de 94,35%. Cependant, un reseau de neurones de 2013 fût capable de battre avec 99.79%. Cela nous fait donc deux reference pour comparer notre reseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep learning</h2>\n",
    "    \n",
    "Comme le signal l'auteur, le fait que l'on ne connaisse pas les poids et les biais donner a nos algorithmes crée un  phénomène souvent repprocher au methode de machine learning l'aspect de boite noire et prend des données et qui en renvoie d'autre, diminuant ainsi notre compréhention que l'on peu en tirer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous permettre de visualiser le concept de \"reseau de neurones profond\", il nous présente l'idée de comment ont pourrait chercher a resoudre la question \"l'image contient t'elle un visage?\" par un reseau de neurone. \n",
    "Ainsi le problème pourrait etre decouper en de nombreuses sous-questions tel que \"un nez est t'il présent?\" et donc pour chacune de ses sous-question un sous-reseau pour y repondre au sein de grand reseau composer de nombreuse couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de poursuivre l'auteur a souhaiter permettre aux lecteurs la possiblilité de d'ouvrir cette \"boite noire\" de d'aider et d'accompagner le lecteur dans la compréhention de l'algorime de rétropropagation un des pilliers permettrant avoir des reseaux plus rapides, plus large et donc plus puissant. \n",
    "Cependant nous l'allons pas tenter ce chapitre qui bien que interressant, il est beaucoup orienter \"lourd\" mathématiquement pour etre expliquer aussi aisaiment que l'auteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importation de Network2, une amelioration de la précédente classe</h2>\n",
    "\n",
    "Avec cette nouvelle classe l'auteur apporte des modifications a la classe network précédente pour integrer des donnée associer a la rétropropagation avec une classe associer a la fonction de Cout et une modification de comment sont initialiser les poids ainsi que les biais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous présenter sa nouvelle classe l'auteur nous propose d'experimenter une situation où ont souhaite a resoudre un nouveau problème depuit le debut sans qu'on est les pistes initial sur la structure de notre reseau et de ses paramètres. Pour cela, il propose a commencer simple avec un reseau sans couche cacher et une petite quantité de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 58 / 100\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 62 / 100\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 75 / 100\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 73 / 100\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 74 / 100\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 68 / 100\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 66 / 100\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 74 / 100\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 79 / 100\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 66 / 100\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 82 / 100\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 75 / 100\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 77 / 100\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 80 / 100\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 75 / 100\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 76 / 100\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 79 / 100\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 68 / 100\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 66 / 100\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 72 / 100\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 73 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "hidden_layer1=20 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=30 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=1.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "lmbda2 = 10.0\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net2 = n2.Network([input_layer,hidden_layer1, output_layer],cost=n2.CrossEntropyCost)\n",
    "net2.large_weight_initializer()\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net2.SGD(training_data[:1000], epoch, size_batch, learning_rate, lmbda = lmbda2,evaluation_data=validation_data[:100],monitor_evaluation_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Suite a des tests d'alterations des hyper-paramètre selon les consignes de l'auteur, nous pouvons obtenir plus rapidement des resultats sur nos changement des paramètres mais cependant cela nous oriente pas encore suffisament sur ce qu'ils devrait etre pour avoir des resultat optimaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/multiple_eta.png\" alt=\"Drawing\" style=\"width: 400px;height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En observant le graphique générer par multiple_eta.py ci-dessus, on peu remarquer que les learning rate 0.025 et 0.25 offre une bonne approche et pourrait servire de base pour la suite de nos experimentations. \n",
    "Par la suite, l'auteur nous conseille d'ajuster notre learning_rate en reproduisant cette meme logique d'observer les resultats de l'evolution du cout. Ces recherches l'on mener a des valeurs de l'ordre de 0.25 et 0.5 fonctionnaient bien. \n",
    "\n",
    "Suite a cela, nous etions davantage libre d'experimenter avec les différentes pistes pour les hyper-paramètre afin de viser les 99%.\n",
    "Pour cela, nous avons repéter un nouvel apprentisage en alterant progressivement nos valeur sur le learning_rate (0.25, ... , 0.15), le lambda(10.0, ..., 0.4), les size_bath (1,10,100), les epoch(30,60,90) ainsi qu'un early_stopping avec un learnin_schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9199 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9199\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9351 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9351\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9439 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9439\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9484 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9484\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9563 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9563\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9571\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9613 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9613\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9641 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9641\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9652\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9674 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9674\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9691 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9691\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9692 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9692\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9694 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9694\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9722 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9722\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9727 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9727\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9735 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9735\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9736 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9736\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9718 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9749 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9749\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9752 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9752\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9758 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9758\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9740 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9763 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9763\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9757 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9758 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9747 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9754 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9767 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9767\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9771 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9771\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9780 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9780\n",
      "Epoch 30 training complete\n",
      "Accuracy on evaluation data: 9780 / 10000\n",
      "\n",
      "Epoch 31 training complete\n",
      "Accuracy on evaluation data: 9771 / 10000\n",
      "\n",
      "Epoch 32 training complete\n",
      "Accuracy on evaluation data: 9773 / 10000\n",
      "\n",
      "Epoch 33 training complete\n",
      "Accuracy on evaluation data: 9784 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9784\n",
      "Epoch 34 training complete\n",
      "Accuracy on evaluation data: 9772 / 10000\n",
      "\n",
      "Epoch 35 training complete\n",
      "Accuracy on evaluation data: 9771 / 10000\n",
      "\n",
      "Epoch 36 training complete\n",
      "Accuracy on evaluation data: 9787 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9787\n",
      "Epoch 37 training complete\n",
      "Accuracy on evaluation data: 9770 / 10000\n",
      "\n",
      "Epoch 38 training complete\n",
      "Accuracy on evaluation data: 9788 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9788\n",
      "Epoch 39 training complete\n",
      "Accuracy on evaluation data: 9789 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9789\n",
      "Epoch 40 training complete\n",
      "Accuracy on evaluation data: 9772 / 10000\n",
      "\n",
      "Epoch 41 training complete\n",
      "Accuracy on evaluation data: 9801 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9801\n",
      "Epoch 42 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "\n",
      "Epoch 43 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "\n",
      "Epoch 44 training complete\n",
      "Accuracy on evaluation data: 9784 / 10000\n",
      "\n",
      "Epoch 45 training complete\n",
      "Accuracy on evaluation data: 9783 / 10000\n",
      "\n",
      "Epoch 46 training complete\n",
      "Accuracy on evaluation data: 9777 / 10000\n",
      "\n",
      "Epoch 47 training complete\n",
      "Accuracy on evaluation data: 9783 / 10000\n",
      "\n",
      "Epoch 48 training complete\n",
      "Accuracy on evaluation data: 9784 / 10000\n",
      "\n",
      "Epoch 49 training complete\n",
      "Accuracy on evaluation data: 9769 / 10000\n",
      "\n",
      "Epoch 50 training complete\n",
      "Accuracy on evaluation data: 9791 / 10000\n",
      "\n",
      "Epoch 51 training complete\n",
      "Accuracy on evaluation data: 9773 / 10000\n",
      "\n",
      "Learning_schedule: Change of learning_rate: 0.08 , limit reduction 0.0\n",
      "Epoch 52 training complete\n",
      "Accuracy on evaluation data: 9790 / 10000\n",
      "\n",
      "Epoch 53 training complete\n",
      "Accuracy on evaluation data: 9781 / 10000\n",
      "\n",
      "Epoch 54 training complete\n",
      "Accuracy on evaluation data: 9794 / 10000\n",
      "\n",
      "Epoch 55 training complete\n",
      "Accuracy on evaluation data: 9794 / 10000\n",
      "\n",
      "Epoch 56 training complete\n",
      "Accuracy on evaluation data: 9800 / 10000\n",
      "\n",
      "Epoch 57 training complete\n",
      "Accuracy on evaluation data: 9798 / 10000\n",
      "\n",
      "Epoch 58 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9804\n",
      "Epoch 59 training complete\n",
      "Accuracy on evaluation data: 9795 / 10000\n",
      "\n",
      "Epoch 60 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 61 training complete\n",
      "Accuracy on evaluation data: 9797 / 10000\n",
      "\n",
      "Epoch 62 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9805\n",
      "Epoch 63 training complete\n",
      "Accuracy on evaluation data: 9800 / 10000\n",
      "\n",
      "Epoch 64 training complete\n",
      "Accuracy on evaluation data: 9806 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9806\n",
      "Epoch 65 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Epoch 66 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 67 training complete\n",
      "Accuracy on evaluation data: 9802 / 10000\n",
      "\n",
      "Epoch 68 training complete\n",
      "Accuracy on evaluation data: 9810 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9810\n",
      "Epoch 69 training complete\n",
      "Accuracy on evaluation data: 9795 / 10000\n",
      "\n",
      "Epoch 70 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Epoch 71 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 72 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 73 training complete\n",
      "Accuracy on evaluation data: 9788 / 10000\n",
      "\n",
      "Epoch 74 training complete\n",
      "Accuracy on evaluation data: 9801 / 10000\n",
      "\n",
      "Epoch 75 training complete\n",
      "Accuracy on evaluation data: 9794 / 10000\n",
      "\n",
      "Epoch 76 training complete\n",
      "Accuracy on evaluation data: 9798 / 10000\n",
      "\n",
      "Epoch 77 training complete\n",
      "Accuracy on evaluation data: 9811 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9811\n",
      "Epoch 78 training complete\n",
      "Accuracy on evaluation data: 9797 / 10000\n",
      "\n",
      "Epoch 79 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Epoch 80 training complete\n",
      "Accuracy on evaluation data: 9812 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9812\n",
      "Epoch 81 training complete\n",
      "Accuracy on evaluation data: 9790 / 10000\n",
      "\n",
      "Epoch 82 training complete\n",
      "Accuracy on evaluation data: 9814 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9814\n",
      "Epoch 83 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 84 training complete\n",
      "Accuracy on evaluation data: 9802 / 10000\n",
      "\n",
      "Epoch 85 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 86 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 87 training complete\n",
      "Accuracy on evaluation data: 9799 / 10000\n",
      "\n",
      "Epoch 88 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 89 training complete\n",
      "Accuracy on evaluation data: 9811 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=90 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=0.16 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "lmbda2 = 5.0\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net2 = n2.Network([input_layer,hidden_layer1, output_layer],cost=n2.CrossEntropyCost)\n",
    "net2.large_weight_initializer()\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net2.SGD(training_data, epoch, size_batch, learning_rate, lmbda = lmbda2,evaluation_data=validation_data,monitor_evaluation_accuracy=True,early_stopping_n=10,learning_schedule=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malgré nos essai répété, nous n'ons pu atteindre en grand maximum un score de <b>98.14%</b>, avec des valeurs aux alentours de : epoch ~= 60 , learning_rate ~= 15-16 , lambda ~=0.5. , early=stopping ~= 5-10 , learning_schedule = True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusion</h1>\n",
    "Pour conclure sur nos éxpérimentations qui nous ont mener a un score de <b>98.14%</b>, une valeur proche des 99% espérer,avec un reseau de neurone a 3 couches [784,100,20] a la fin du chapitre 3 qui a été notre point d'arret par manque de temps. \n",
    "Nous en avons appris beaucoup sur la construction des reseaux de neurones. En effet, malgré l'accompagnement de l'auteur et de ses explications sur l'envers du decors, nous pouvons facilement reconnaitre la complexité que peu représenté la constrution d'un reseau de neurone adapté a l'objectif par sa structures, ses algorithmes ou encore ses hyper-paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
