{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rapport de Deep Learning</h1>\n",
    "<h2>Implémentation d'un réseau de neurones pour classifier des chiffres en écriture manuscrite</h2>\n",
    "<h3> Yannis Amirat - Cyril Lagelée - ESIEE Paris 2021 - E4FI</h3>\n",
    "\n",
    "\n",
    "\n",
    "Dans ce projet, nous suivons les instructions d'un livre numérique situé à l'adresse http://neuralnetworksanddeeplearning.com afin de comprendre comment est implémenté un réseau de neurones de Deep Learning afin de classifier des chiffres en écriture manuscrite. \n",
    "Nous allons transcrire notre progression au cours de ce projet dans ce jupyter notebook afin que le lecteur puisse reproduire notre parcours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au cours de ce livre, après une introduction à la structure des réseaux de neurones et à une implémentation de descente de gradient, l'auteur nous redirige vers son git contenant les codes afin de suivre le déroulement de cette lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importation des codes fournies pour le projet</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import mnist_loader\n",
    "import network\n",
    "import mnist_svm\n",
    "import network2 as n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy est une bibliothèque d'outils mathématiques commune à Python. \n",
    "\n",
    "Random est un outil permettant de générer des éléments pseudo-aléatoire. \n",
    "\n",
    "mnist_loader est un outil permettant de charger les données avec laquelle on va travailler provenant de Mixed National Institute of Standards and Technology. Ces données sont des scans de chiffres écrits à la main de 784 pixels.\n",
    "\n",
    "Network est une classe représentant une première forme d'implémentation d'un réseau de neurone. Dans cette classe est présent une implémentation d'un algorithme de descente de gradient stochastique avec un poids et un biais initialisés aléatoirement au démarrage de l'algorithme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initiation au réseau de neurones</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous faire comprendre comment fonctionne cette implémentation de réseau de neurones, l'auteur nous fait construire un réseau de neurone en guise d'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9080 / 10000\n",
      "Epoch 1: 9192 / 10000\n",
      "Epoch 2: 9334 / 10000\n",
      "Epoch 3: 9368 / 10000\n",
      "Epoch 4: 9393 / 10000\n",
      "Epoch 5: 9402 / 10000\n",
      "Epoch 6: 9426 / 10000\n",
      "Epoch 7: 9467 / 10000\n",
      "Epoch 8: 9451 / 10000\n",
      "Epoch 9: 9434 / 10000\n",
      "Epoch 10: 9479 / 10000\n",
      "Epoch 11: 9473 / 10000\n",
      "Epoch 12: 9415 / 10000\n",
      "Epoch 13: 9460 / 10000\n",
      "Epoch 14: 9468 / 10000\n",
      "Epoch 15: 9480 / 10000\n",
      "Epoch 16: 9434 / 10000\n",
      "Epoch 17: 9486 / 10000\n",
      "Epoch 18: 9478 / 10000\n",
      "Epoch 19: 9482 / 10000\n",
      "Epoch 20: 9499 / 10000\n",
      "Epoch 21: 9505 / 10000\n",
      "Epoch 22: 9479 / 10000\n",
      "Epoch 23: 9461 / 10000\n",
      "Epoch 24: 9511 / 10000\n",
      "Epoch 25: 9416 / 10000\n",
      "Epoch 26: 9502 / 10000\n",
      "Epoch 27: 9501 / 10000\n",
      "Epoch 28: 9490 / 10000\n",
      "Epoch 29: 9503 / 10000\n"
     ]
    }
   ],
   "source": [
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "hidden_layer1=30 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=30 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=3.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le livre, l'auteur obtient un pourcentage de 95,42% à son meilleur epoch qu'il trouve prometteur. De notre côté, nous obtenons un résultat légèrement différent en raison des biais et des poids aléatoires à l'initialisation. \n",
    "A l'epoch 22, on obtient 94.71% ce qui représente en effet un bon résultat de reconnaissance de chiffres.\n",
    "Suite à cela, l'auteur nous propose de tester avec 100 neurones sur la couche cachée et différents learning rate afin d'en observer les résultats. \n",
    "\n",
    "Nous commençons avec un learning rate de 0.001 que l'on va progressivement faire augmenter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1211 / 10000\n",
      "Epoch 1: 1189 / 10000\n",
      "Epoch 2: 1201 / 10000\n",
      "Epoch 3: 1220 / 10000\n",
      "Epoch 4: 1253 / 10000\n",
      "Epoch 5: 1288 / 10000\n",
      "Epoch 6: 1322 / 10000\n",
      "Epoch 7: 1375 / 10000\n",
      "Epoch 8: 1415 / 10000\n",
      "Epoch 9: 1450 / 10000\n",
      "Epoch 10: 1483 / 10000\n",
      "Epoch 11: 1509 / 10000\n",
      "Epoch 12: 1520 / 10000\n",
      "Epoch 13: 1552 / 10000\n",
      "Epoch 14: 1583 / 10000\n",
      "Epoch 15: 1599 / 10000\n",
      "Epoch 16: 1622 / 10000\n",
      "Epoch 17: 1644 / 10000\n",
      "Epoch 18: 1668 / 10000\n",
      "Epoch 19: 1694 / 10000\n",
      "Epoch 20: 1737 / 10000\n",
      "Epoch 21: 1767 / 10000\n",
      "Epoch 22: 1798 / 10000\n",
      "Epoch 23: 1823 / 10000\n",
      "Epoch 24: 1870 / 10000\n",
      "Epoch 25: 1898 / 10000\n",
      "Epoch 26: 1939 / 10000\n",
      "Epoch 27: 1974 / 10000\n",
      "Epoch 28: 2016 / 10000\n",
      "Epoch 29: 2062 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.001 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant des paramètres incorrectement défini peuvent causer une lenteur de l'exécution mais aussi de très mauvais résultat. \n",
    "Malgré l'ajout de 70 neurones à notre couche cachée, le meilleur résultat qu'on a obtenu est de l'ordre de 20,62% avec un learning rate de 0.001. \n",
    "L'auteur suggère que notre learning rate est en effet la source de nos faibles résultats et suggère d'effectuer plusieurs exécutions en augmentant le learning rate et d'observer les résultats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1313 / 10000\n",
      "Epoch 1: 1518 / 10000\n",
      "Epoch 2: 1668 / 10000\n",
      "Epoch 3: 1773 / 10000\n",
      "Epoch 4: 1869 / 10000\n",
      "Epoch 5: 1929 / 10000\n",
      "Epoch 6: 1976 / 10000\n",
      "Epoch 7: 2012 / 10000\n",
      "Epoch 8: 2056 / 10000\n",
      "Epoch 9: 2086 / 10000\n",
      "Epoch 10: 2117 / 10000\n",
      "Epoch 11: 2160 / 10000\n",
      "Epoch 12: 2202 / 10000\n",
      "Epoch 13: 2248 / 10000\n",
      "Epoch 14: 2316 / 10000\n",
      "Epoch 15: 2438 / 10000\n",
      "Epoch 16: 2745 / 10000\n",
      "Epoch 17: 2945 / 10000\n",
      "Epoch 18: 3022 / 10000\n",
      "Epoch 19: 3086 / 10000\n",
      "Epoch 20: 3148 / 10000\n",
      "Epoch 21: 3199 / 10000\n",
      "Epoch 22: 3252 / 10000\n",
      "Epoch 23: 3311 / 10000\n",
      "Epoch 24: 3361 / 10000\n",
      "Epoch 25: 3427 / 10000\n",
      "Epoch 26: 3483 / 10000\n",
      "Epoch 27: 3553 / 10000\n",
      "Epoch 28: 3624 / 10000\n",
      "Epoch 29: 3693 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.01 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.001 à 0.01, nous observons une progression de notre pourcentage d'environ 16.31% qui est dorénavant de 36,93%. Nous supposons qu'une augmentation du learning rate augmentera ce pourcentage de nouveau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 4528 / 10000\n",
      "Epoch 1: 6135 / 10000\n",
      "Epoch 2: 6960 / 10000\n",
      "Epoch 3: 7203 / 10000\n",
      "Epoch 4: 7310 / 10000\n",
      "Epoch 5: 7395 / 10000\n",
      "Epoch 6: 7426 / 10000\n",
      "Epoch 7: 7451 / 10000\n",
      "Epoch 8: 7489 / 10000\n",
      "Epoch 9: 7503 / 10000\n",
      "Epoch 10: 7523 / 10000\n",
      "Epoch 11: 7544 / 10000\n",
      "Epoch 12: 7551 / 10000\n",
      "Epoch 13: 7567 / 10000\n",
      "Epoch 14: 7572 / 10000\n",
      "Epoch 15: 7601 / 10000\n",
      "Epoch 16: 7605 / 10000\n",
      "Epoch 17: 7614 / 10000\n",
      "Epoch 18: 7623 / 10000\n",
      "Epoch 19: 7630 / 10000\n",
      "Epoch 20: 7640 / 10000\n",
      "Epoch 21: 7647 / 10000\n",
      "Epoch 22: 7656 / 10000\n",
      "Epoch 23: 7663 / 10000\n",
      "Epoch 24: 7662 / 10000\n",
      "Epoch 25: 7670 / 10000\n",
      "Epoch 26: 7675 / 10000\n",
      "Epoch 27: 7679 / 10000\n",
      "Epoch 28: 7684 / 10000\n",
      "Epoch 29: 7694 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=0.1 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.01 à 0.1, nous observons encore une progression significative de notre pourcentage d'environ 28% qui est dorénavant de 74.96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7688 / 10000\n",
      "Epoch 1: 8248 / 10000\n",
      "Epoch 2: 8376 / 10000\n",
      "Epoch 3: 9233 / 10000\n",
      "Epoch 4: 9336 / 10000\n",
      "Epoch 5: 9390 / 10000\n",
      "Epoch 6: 9403 / 10000\n",
      "Epoch 7: 9435 / 10000\n",
      "Epoch 8: 9440 / 10000\n",
      "Epoch 9: 9452 / 10000\n",
      "Epoch 10: 9477 / 10000\n",
      "Epoch 11: 9494 / 10000\n",
      "Epoch 12: 9512 / 10000\n",
      "Epoch 13: 9526 / 10000\n",
      "Epoch 14: 9516 / 10000\n",
      "Epoch 15: 9528 / 10000\n",
      "Epoch 16: 9521 / 10000\n",
      "Epoch 17: 9539 / 10000\n",
      "Epoch 18: 9542 / 10000\n",
      "Epoch 19: 9563 / 10000\n",
      "Epoch 20: 9562 / 10000\n",
      "Epoch 21: 9554 / 10000\n",
      "Epoch 22: 9561 / 10000\n",
      "Epoch 23: 9559 / 10000\n",
      "Epoch 24: 9575 / 10000\n",
      "Epoch 25: 9567 / 10000\n",
      "Epoch 26: 9570 / 10000\n",
      "Epoch 27: 9562 / 10000\n",
      "Epoch 28: 9566 / 10000\n",
      "Epoch 29: 9566 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=1.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 0.1 à 1.0, nous observons de nouveau une progression de notre pourcentage d'environ 18.81% qui est dorénavant de 95.75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6481 / 10000\n",
      "Epoch 1: 7499 / 10000\n",
      "Epoch 2: 7584 / 10000\n",
      "Epoch 3: 7632 / 10000\n",
      "Epoch 4: 7726 / 10000\n",
      "Epoch 5: 8508 / 10000\n",
      "Epoch 6: 8560 / 10000\n",
      "Epoch 7: 8585 / 10000\n",
      "Epoch 8: 8600 / 10000\n",
      "Epoch 9: 8615 / 10000\n",
      "Epoch 10: 8634 / 10000\n",
      "Epoch 11: 8634 / 10000\n",
      "Epoch 12: 8649 / 10000\n",
      "Epoch 13: 8663 / 10000\n",
      "Epoch 14: 8671 / 10000\n",
      "Epoch 15: 8662 / 10000\n",
      "Epoch 16: 8677 / 10000\n",
      "Epoch 17: 8715 / 10000\n",
      "Epoch 18: 9580 / 10000\n",
      "Epoch 19: 9586 / 10000\n",
      "Epoch 20: 9592 / 10000\n",
      "Epoch 21: 9584 / 10000\n",
      "Epoch 22: 9581 / 10000\n",
      "Epoch 23: 9596 / 10000\n",
      "Epoch 24: 9593 / 10000\n",
      "Epoch 25: 9606 / 10000\n",
      "Epoch 26: 9598 / 10000\n",
      "Epoch 27: 9609 / 10000\n",
      "Epoch 28: 9614 / 10000\n",
      "Epoch 29: 9603 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=2.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 1.0 à 2.0, nous observons une nouvelle fois une progression légère de notre pourcentage de moins de 1% qui est dorénavant de 96.14%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 5575 / 10000\n",
      "Epoch 1: 6628 / 10000\n",
      "Epoch 2: 6704 / 10000\n",
      "Epoch 3: 6773 / 10000\n",
      "Epoch 4: 7508 / 10000\n",
      "Epoch 5: 7617 / 10000\n",
      "Epoch 6: 7691 / 10000\n",
      "Epoch 7: 7716 / 10000\n",
      "Epoch 8: 7720 / 10000\n",
      "Epoch 9: 7780 / 10000\n",
      "Epoch 10: 7779 / 10000\n",
      "Epoch 11: 7743 / 10000\n",
      "Epoch 12: 7840 / 10000\n",
      "Epoch 13: 7943 / 10000\n",
      "Epoch 14: 7855 / 10000\n",
      "Epoch 15: 7887 / 10000\n",
      "Epoch 16: 8131 / 10000\n",
      "Epoch 17: 8676 / 10000\n",
      "Epoch 18: 8679 / 10000\n",
      "Epoch 19: 8680 / 10000\n",
      "Epoch 20: 8685 / 10000\n",
      "Epoch 21: 8685 / 10000\n",
      "Epoch 22: 8687 / 10000\n",
      "Epoch 23: 8683 / 10000\n",
      "Epoch 24: 8686 / 10000\n",
      "Epoch 25: 8682 / 10000\n",
      "Epoch 26: 8697 / 10000\n",
      "Epoch 27: 8689 / 10000\n",
      "Epoch 28: 8693 / 10000\n",
      "Epoch 29: 8705 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=3.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, hidden_layer1, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En changeant le learning rate de 2.0 à 3.0, nous observons une régression de notre pourcentage d'environ 9% qui est dorénavant de 87.05%. \n",
    "\n",
    "\n",
    "L'hypothèse que l'on peut émettre sur ces résultats est que les variations de learning rate, de poids aléatoire et de biais aléatoire peuvent avoir un impact significatif sur les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Exercice\n",
    "\n",
    "Essayez de crée un réseau avec deux layers, un input et un output sans réseau cacher avec 784 et 10 neurones chacun. Entrainez le réseau avec la descente de gradient stocastique. Quelle précision de classification pouvez-vous obtenir ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6070 / 10000\n",
      "Epoch 1: 6263 / 10000\n",
      "Epoch 2: 6276 / 10000\n",
      "Epoch 3: 6309 / 10000\n",
      "Epoch 4: 6308 / 10000\n",
      "Epoch 5: 6326 / 10000\n",
      "Epoch 6: 6325 / 10000\n",
      "Epoch 7: 6315 / 10000\n",
      "Epoch 8: 6341 / 10000\n",
      "Epoch 9: 6332 / 10000\n",
      "Epoch 10: 6349 / 10000\n",
      "Epoch 11: 6361 / 10000\n",
      "Epoch 12: 6341 / 10000\n",
      "Epoch 13: 6386 / 10000\n",
      "Epoch 14: 7257 / 10000\n",
      "Epoch 15: 7295 / 10000\n",
      "Epoch 16: 8375 / 10000\n",
      "Epoch 17: 8401 / 10000\n",
      "Epoch 18: 8397 / 10000\n",
      "Epoch 19: 8387 / 10000\n",
      "Epoch 20: 8385 / 10000\n",
      "Epoch 21: 8373 / 10000\n",
      "Epoch 22: 8390 / 10000\n",
      "Epoch 23: 8386 / 10000\n",
      "Epoch 24: 8412 / 10000\n",
      "Epoch 25: 8411 / 10000\n",
      "Epoch 26: 8403 / 10000\n",
      "Epoch 27: 8425 / 10000\n",
      "Epoch 28: 8400 / 10000\n",
      "Epoch 29: 9017 / 10000\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "learning_rate=2.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "#Rechargement des données du MNIST, afin de repartir sur un état brute \n",
    "training_data, validation_data, test_data =mnist_loader.load_data_wrapper() \n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net = network.Network([input_layer, output_layer])\n",
    "net.SGD(training_data, epoch, size_batch, learning_rate, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un résultat de 90.17% ce qui est un score plutôt bon pour un réseau composer de deux couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vers des méthodes moins naïves</h1>\n",
    "\n",
    "Dans la suite du livre, l'auteur nous présente l'implémentation de mnist_loader qui est en plus chargé de données, ce programme structure les données afin d'être correctement interprété par le réseau de neurones.\n",
    "\n",
    "Ensuite ce dernier, nous rappelle que notre méthode est naïve et qu'elle manque de méthodologie pour comparer nos résultats. En effet nous pouvons comparer avec notre propre réseau mais l'idéal serait de pouvoir comparer avec d'autres méthodes qui ne sont pas basé sur les réseaux de neurones. Pour cela, il suggère d'utiliser une implémentation d'un des algorithmes les plus connue avec ses paramètres par défauts : support vector machine ou SVM afin de comparer.\n",
    "\n",
    "Ce dernier devrait avoir un résultat d’environ 94.35% par défaut et environ 98.5% lorsque qu'il est bien paramétré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda2\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classifier using an SVM.\n",
      "9435 of 10000 values correct.\n"
     ]
    }
   ],
   "source": [
    "mnist_svm.svm_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme annoncer par l'auteur, avec SVM, nous obtenons un très bon résultat de 97,85%. Un réseau de neurones de 2013 fût capable de battre ce score avec 99.79%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep learning</h2>\n",
    "    \n",
    "Comme le signale l'auteur, le fait que l'on ne connaisse pas les poids et les biais donnés à nos algorithmes crée un phénomène souvent reprocher aux méthodes de machine learning avec l'aspect de boite noire qui prend des données et qui en renvoie d'autre, diminuant ainsi notre compréhension que l'on peut en tirer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous permettre de visualiser le concept de \"réseau de neurones profond\", il nous présente l'idée de comment on pourrait chercher a résoudre la question \"l'image contient-elle un visage ?\" par un réseau de neurone. \n",
    "Ainsi le problème pourrait être découpé en de nombreuses sous-questions tel que \"un nez est-il présent?\" et donc pour chacune de ses sous-question un sous-réseau pour y répondre au sein de grand réseau composés de nombreuses couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de poursuivre l'auteur a souhaité permettre aux lecteurs la possibilité d'ouvrir cette \"boite noire\" afin d'aider et d'accompagner le lecteur dans la compréhension de l'algorithme de rétropropagation d'un des piliers permettant d'avoir des réseaux plus rapides, plus larges et donc plus puissant. \n",
    "Cependant nous n'allons pas tenter ce chapitre qui bien qu’intéressant, il est beaucoup orienter mathématiques pour être expliqué aussi aisément que l'auteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Importation de Network2, une amélioration de la précédente classe</h2>\n",
    "\n",
    "Avec cette nouvelle classe, l'auteur apporte des modifications à la classe network précédente pour intégrer des données associées à la rétropropagation avec une classe associée à la fonction de Cout et une modification de comment sont initialisés les poids ainsi que les biais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de nous présenter sa nouvelle classe, l'auteur nous propose d'expérimenter une situation où on souhaite résoudre un nouveau problème depuis le début sans qu'on ait les pistes initiales sur la structure de notre réseau et de ses paramètres. Pour cela, il nous propose de commencer simplement avec un réseau sans couche caché et une petite quantité de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 58 / 100\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 62 / 100\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 75 / 100\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 73 / 100\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 74 / 100\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 68 / 100\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 66 / 100\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 74 / 100\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 79 / 100\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 66 / 100\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 82 / 100\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 69 / 100\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 75 / 100\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 77 / 100\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 80 / 100\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 75 / 100\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 76 / 100\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 79 / 100\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 68 / 100\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 66 / 100\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 70 / 100\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 72 / 100\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 73 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "hidden_layer1=20 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=30 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=1.0 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "lmbda2 = 10.0\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net2 = n2.Network([input_layer,hidden_layer1, output_layer],cost=n2.CrossEntropyCost)\n",
    "net2.large_weight_initializer()\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net2.SGD(training_data[:1000], epoch, size_batch, learning_rate, lmbda = lmbda2,evaluation_data=validation_data[:100],monitor_evaluation_accuracy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Suite à des tests d'altérations des hyper-paramètre selon les consignes de l'auteur, nous pouvons obtenir plus rapidement des résultats sur les changements de nos paramètres mais cependant cela ne nous oriente pas encore suffisamment vers des résultats optimaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../fig/multiple_eta.png\" alt=\"Drawing\" style=\"width: 400px;height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En observant le graphique généré ci-dessus, on peut remarquer que les learning rate 0.025 et 0.25 offre une bonne approche et pourrait servir de base pour la suite de nos expérimentations. \n",
    "Par la suite, l'auteur nous conseille d'ajuster notre learning_rate en reproduisant cette même logique afin d'observer les résultats de l'évolution du cout. Ces recherches l'ont mené a des valeurs de l'ordre de 0.25 et 0.5 qui fonctionnaient bien. \n",
    "\n",
    "Suite à cela, nous étions libres d'expérimenter davantage avec les différentes pistes pour les hyper paramètre afin de viser le score de 99%.\n",
    "Pour cela, nous avons répétés un nouvel apprentissage en altérant progressivement nos valeur sur le learning_rate (0.25, ... , 0.15), le lambda(10.0, ..., 0.4), les size_batch (1,10,100), les epoch(30,60,90) ainsi que l'early_stopping et le learnin_schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9199 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9199\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9351 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9351\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9439 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9439\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9484 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9484\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9563 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9563\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9571\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9613 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9613\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9641 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9641\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9652\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9674 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9674\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9691 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9691\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9692 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9692\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9694 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9694\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9722 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9722\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9727 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9727\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9735 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9735\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9736 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9736\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9718 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9749 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9749\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9752 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9752\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9758 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9758\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9740 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9763 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9763\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9757 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9758 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9747 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9754 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9767 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9767\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9771 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9771\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9780 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9780\n",
      "Epoch 30 training complete\n",
      "Accuracy on evaluation data: 9780 / 10000\n",
      "\n",
      "Epoch 31 training complete\n",
      "Accuracy on evaluation data: 9771 / 10000\n",
      "\n",
      "Epoch 32 training complete\n",
      "Accuracy on evaluation data: 9773 / 10000\n",
      "\n",
      "Epoch 33 training complete\n",
      "Accuracy on evaluation data: 9784 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9784\n",
      "Epoch 34 training complete\n",
      "Accuracy on evaluation data: 9772 / 10000\n",
      "\n",
      "Epoch 35 training complete\n",
      "Accuracy on evaluation data: 9771 / 10000\n",
      "\n",
      "Epoch 36 training complete\n",
      "Accuracy on evaluation data: 9787 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9787\n",
      "Epoch 37 training complete\n",
      "Accuracy on evaluation data: 9770 / 10000\n",
      "\n",
      "Epoch 38 training complete\n",
      "Accuracy on evaluation data: 9788 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9788\n",
      "Epoch 39 training complete\n",
      "Accuracy on evaluation data: 9789 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9789\n",
      "Epoch 40 training complete\n",
      "Accuracy on evaluation data: 9772 / 10000\n",
      "\n",
      "Epoch 41 training complete\n",
      "Accuracy on evaluation data: 9801 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9801\n",
      "Epoch 42 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "\n",
      "Epoch 43 training complete\n",
      "Accuracy on evaluation data: 9786 / 10000\n",
      "\n",
      "Epoch 44 training complete\n",
      "Accuracy on evaluation data: 9784 / 10000\n",
      "\n",
      "Epoch 45 training complete\n",
      "Accuracy on evaluation data: 9783 / 10000\n",
      "\n",
      "Epoch 46 training complete\n",
      "Accuracy on evaluation data: 9777 / 10000\n",
      "\n",
      "Epoch 47 training complete\n",
      "Accuracy on evaluation data: 9783 / 10000\n",
      "\n",
      "Epoch 48 training complete\n",
      "Accuracy on evaluation data: 9784 / 10000\n",
      "\n",
      "Epoch 49 training complete\n",
      "Accuracy on evaluation data: 9769 / 10000\n",
      "\n",
      "Epoch 50 training complete\n",
      "Accuracy on evaluation data: 9791 / 10000\n",
      "\n",
      "Epoch 51 training complete\n",
      "Accuracy on evaluation data: 9773 / 10000\n",
      "\n",
      "Learning_schedule: Change of learning_rate: 0.08 , limit reduction 0.0\n",
      "Epoch 52 training complete\n",
      "Accuracy on evaluation data: 9790 / 10000\n",
      "\n",
      "Epoch 53 training complete\n",
      "Accuracy on evaluation data: 9781 / 10000\n",
      "\n",
      "Epoch 54 training complete\n",
      "Accuracy on evaluation data: 9794 / 10000\n",
      "\n",
      "Epoch 55 training complete\n",
      "Accuracy on evaluation data: 9794 / 10000\n",
      "\n",
      "Epoch 56 training complete\n",
      "Accuracy on evaluation data: 9800 / 10000\n",
      "\n",
      "Epoch 57 training complete\n",
      "Accuracy on evaluation data: 9798 / 10000\n",
      "\n",
      "Epoch 58 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9804\n",
      "Epoch 59 training complete\n",
      "Accuracy on evaluation data: 9795 / 10000\n",
      "\n",
      "Epoch 60 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 61 training complete\n",
      "Accuracy on evaluation data: 9797 / 10000\n",
      "\n",
      "Epoch 62 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9805\n",
      "Epoch 63 training complete\n",
      "Accuracy on evaluation data: 9800 / 10000\n",
      "\n",
      "Epoch 64 training complete\n",
      "Accuracy on evaluation data: 9806 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9806\n",
      "Epoch 65 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Epoch 66 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 67 training complete\n",
      "Accuracy on evaluation data: 9802 / 10000\n",
      "\n",
      "Epoch 68 training complete\n",
      "Accuracy on evaluation data: 9810 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9810\n",
      "Epoch 69 training complete\n",
      "Accuracy on evaluation data: 9795 / 10000\n",
      "\n",
      "Epoch 70 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Epoch 71 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 72 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 73 training complete\n",
      "Accuracy on evaluation data: 9788 / 10000\n",
      "\n",
      "Epoch 74 training complete\n",
      "Accuracy on evaluation data: 9801 / 10000\n",
      "\n",
      "Epoch 75 training complete\n",
      "Accuracy on evaluation data: 9794 / 10000\n",
      "\n",
      "Epoch 76 training complete\n",
      "Accuracy on evaluation data: 9798 / 10000\n",
      "\n",
      "Epoch 77 training complete\n",
      "Accuracy on evaluation data: 9811 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9811\n",
      "Epoch 78 training complete\n",
      "Accuracy on evaluation data: 9797 / 10000\n",
      "\n",
      "Epoch 79 training complete\n",
      "Accuracy on evaluation data: 9804 / 10000\n",
      "\n",
      "Epoch 80 training complete\n",
      "Accuracy on evaluation data: 9812 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9812\n",
      "Epoch 81 training complete\n",
      "Accuracy on evaluation data: 9790 / 10000\n",
      "\n",
      "Epoch 82 training complete\n",
      "Accuracy on evaluation data: 9814 / 10000\n",
      "\n",
      "Early-stopping: Best so far 9814\n",
      "Epoch 83 training complete\n",
      "Accuracy on evaluation data: 9803 / 10000\n",
      "\n",
      "Epoch 84 training complete\n",
      "Accuracy on evaluation data: 9802 / 10000\n",
      "\n",
      "Epoch 85 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 86 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 87 training complete\n",
      "Accuracy on evaluation data: 9799 / 10000\n",
      "\n",
      "Epoch 88 training complete\n",
      "Accuracy on evaluation data: 9805 / 10000\n",
      "\n",
      "Epoch 89 training complete\n",
      "Accuracy on evaluation data: 9811 / 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_layer=784 #Le nombre de neurones dans la couche d'entrée du réseau (784 pour le nombre de pixels des images)\n",
    "hidden_layer1=100 #nombre de neurones dans la 1ère couche cachée\n",
    "output_layer=10 #nombre de neurones dans la couche de sortie du réseau (10 pour les chiffres de 0 à 9)\n",
    "\n",
    "epoch=90 #nombre de fois que l'algorithme travaille sur le dataset d'entrainement\n",
    "size_batch=10 #nombre d'échantillons à traiter avant de mettre à jour le modèle interne du réseau\n",
    "learning_rate=0.16 #distance parcourut à chaque étape pour la descente de gradient\n",
    "\n",
    "lmbda2 = 5.0\n",
    "\n",
    "#Chargement des données du MNIST \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "#Création du réseau de neurones avec nos paramètres\n",
    "net2 = n2.Network([input_layer,hidden_layer1, output_layer],cost=n2.CrossEntropyCost)\n",
    "net2.large_weight_initializer()\n",
    "#Application de la descente de gradient stochastique \n",
    "# sur notre réseau de neurones avec comparaison sur des données de test pour évaluer la précision du réseau \n",
    "evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net2.SGD(training_data, epoch, size_batch, learning_rate, lmbda = lmbda2,evaluation_data=validation_data,monitor_evaluation_accuracy=True,early_stopping_n=10,learning_schedule=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malgré nos essais répétés, nous n'avons pu atteindre au grand maximum un score de <b>98.14%</b>, avec les valeurs suivantes : epoch ~= 60 , learning_rate ~= 15-16 , lambda ~=0.5. , early=stopping ~= 5-10 , learning_schedule = True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusion</h1>\n",
    "Pour conclure, nos expérimentations nous ont mené à un score de <b>98.14%</b> qui est une valeur proche des 99% espérés, avec un réseau de neurone à 3 couches [784,100,20] ce qui correspond à la fin du chapitre 3. \n",
    "Nous avons appris beaucoup sur la construction des réseaux de neurones. En effet, malgré l'accompagnement de l'auteur et de ses explications sur le fonctionnement du deep learning, nous pouvons facilement reconnaitre la complexité que peut représenter la construction d'un réseau de neurones adapté en fonction de sa structure, ses algorithmes ou encore ses hyper paramètres."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
